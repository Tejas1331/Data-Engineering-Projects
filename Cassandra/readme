

## Data Pipeline with Kafka and Cassandra Integration
Project Overview

This data pipeline is designed to seamlessly transfer data from a CSV file to a Kafka cluster hosted on Confluent Cloud. The pipeline incorporates producer and consumer components, utilizing Apache Kafka for real-time data streaming and processing, and ultimately storing the processed data in a Cassandra database.

Key Features
CSV to Kafka: Data is ingested from a CSV file and sent to a Kafka cluster using a custom producer code.
Real-time Processing: Data is consumed from Kafka and subjected to real-time processing using a consumer code.
Data Storage: Processed data is stored in a Cassandra database, enabling efficient and scalable data management.
Scalability: The pipeline can handle large volumes of data and is highly scalable to meet changing data demands.

Technologies Used
Apache Kafka
Confluent Cloud
Cassandra
CSV data ingestion
Python (for producer and consumer components)
